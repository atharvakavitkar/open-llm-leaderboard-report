# Open LLM Leaderboard Report Revision

#### 2023.06.19

- I have come to the conclusion that the current metrics being used for evaluation cannot be trusted, as I initially suspected. The recent report highlights the reason for the growing gap between the Open LLM community and the private LLM community, such as OpenAI. It suggests that the use of quantitative evaluation metrics that are difficult to accurately assess qualitatively also plays a role in this.
- Developing a good LLM, like those in the private sector, requires sufficient resources and an organization capable of handling legal issues.
- Furthermore, as I initially anticipated, it seems true that the Open LLM leaderboard is tailored towards marketing purposes and aligned with Falcon's release.
- Related Paper: [Exploring the MIT Mathematics and EECS Curriculum Using Large Language Models](https://arxiv.org/abs/2306.08997)

![](assets/20230619/totalplot.png)


#### 2023.06.10
- The system is currently overheated, with the number of pending models exceeding 400. As a result, the performance of the models is starting to differ by decimal points. However, as initially suspected, the performance metrics may be biased towards Falcon, so it is advisable to use them as rough reference indicators rather than absolute quantitative measures. It has been observed that some models, despite having superior qualitative results, rank lower.
- To alleviate the overheating of the leaderboard, the number of models to be visualized is adjusted to the top 70.
![](assets/20230610/totalplot.png)

<br>

#### 2023.05.31
- With the addition of several key models and improvements in the performance of large-scale models, the changes in the number of parameters become noticeable.
![](assets/20230531/totalplot.png)

<br>



#### 2023.05.26
- Although it is difficult to completely dismiss the idea that some datasets or metrics may favor Hugging Face's Falcon, it still provides a good overall estimation of model performance.
![](assets/20230526/totalplot.png)
<br>



#### 2023.05.23
- It provides a good overall estimation of model performance.
![](assets/20230523/totalplot.png)
<br>
